{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been working with data that can fit on a local server (0-8 GB)\n",
    "\n",
    "What if we want to use a larger dataset?\n",
    "\n",
    "-- We can use a SQL database to move storage onto hard drive instead of RAM\n",
    "\n",
    "-- We can use a distributed database system that stores data on multiple computers\n",
    "\n",
    "A distributed process has access to the computing power of multiple machines, making it easier to scale tasks out to many lower CPU machines than it is to scale up to a single machine with high CPU.\n",
    "\n",
    "Distributed systems are also fault tolerant, so if one machine fails the networks can still go on.\n",
    "\n",
    "Hadoop uses the Hadoop Distributed File System (HDFS) to distribute very large files across multiple machines.\n",
    "\n",
    "HDFS duplicates blocks of data for fault tolerance (standard is 3x replication in 128 MB chunks each), then uses MapReduce to perform tasks on that data.  Using smaller blocks allows Hadoop to gain more parallelization during processing.\n",
    "\n",
    "MapReduce is a way of splitting a computation task to a distributed file set (e.g. HDFS) and consists of a Job Tracker and multiple Task Trackers.\n",
    "\n",
    "The Job Tracker sends code to the Task Trackers, which in turn allocate CPU and memory for the tasks and monitor the tasks on the worker nodes.\n",
    "\n",
    "There are really two distinct parts:\n",
    "\n",
    "-- Using HDFS to distribute large datasets\n",
    "\n",
    "-- Using MapReduce to distribute a computational task to a distributed dataset\n",
    "\n",
    "Spark improves on the concepts of using distribution and extends beyond MapReduce for greater functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark:**\n",
    "\n",
    "Spark was released in 2013 and is a flexible alternative to MapReduce.\n",
    "\n",
    "Spark can use data stored in a variety of formats, including Cassandra, AWS S3, HDFS, and more.\n",
    "\n",
    "Spark can perform operations up to 100x faster than MapReduce.\n",
    "\n",
    "-- This is achieved by keeping most data in-memory after each transformation (unless memory is full) instead of writing most data to disk after each map and reduce operation (as MapReduce does).\n",
    "\n",
    "At the core of Spark is the Resilient Distributed Dataset (RDD)\n",
    "\n",
    "RDDs have 4 main features:\n",
    "\n",
    "-- Distributed collection of data\n",
    "\n",
    "-- Fault-tolerant\n",
    "\n",
    "-- Partitioned by parallel operations\n",
    "\n",
    "-- Flexible with many data sources\n",
    "\n",
    "Spark has a driver program, which operates a Spark Context, which communicates with a cluster manager, which then communicates with the worker nodes.\n",
    "\n",
    "RDDs are immutable, lazily evaluated, and cacheable\n",
    "\n",
    "There are two types of RDD operations: Transformations and Actions\n",
    "\n",
    "Basic actions are First, Collect, Count, and Take\n",
    "\n",
    "-- Collect returns all elements of an RDD as an array at the driver program\n",
    "\n",
    "-- Count returns the number of elements in an RDD\n",
    "\n",
    "-- First returns first element in an RDD\n",
    "\n",
    "-- Take returns an array with the first n elements of the RDD\n",
    "\n",
    "Basic transformations are Filter, Map, and FlatMap\n",
    "\n",
    "-- RDD.filter() applies a function to each element and returns elements that evaluate to true\n",
    "\n",
    "-- RDD.map() transforms each element and preserves # of elements, similar to .apply() in pandas\n",
    "\n",
    "-- RDD.flatMap() transforms each element into 0 to n elements and changes # of elements\n",
    "\n",
    "Map() would grab the first letter in a list of words, while FlatMap() would transform a corpus into a list of words.\n",
    "\n",
    "Often RDDs will hold their values in tuples known as Pair RDDs.  This offers better partitioning of data and leads to functionality based on reduction.\n",
    "\n",
    "Other actions:\n",
    "\n",
    "-- Reduce() aggregates RDD elements using a function that returns a single element\n",
    "\n",
    "-- ReduceByKey() aggregates pair RDD elements using a function that returns a pair RDD\n",
    "\n",
    "-- Both of these operate similar to Group By operations\n",
    "\n",
    "The Spark ecosystem now includes:\n",
    "\n",
    "-- Spark SQL\n",
    "\n",
    "-- Spark DataFrames\n",
    "\n",
    "-- MLib\n",
    "\n",
    "-- GraphX\n",
    "\n",
    "-- Spark Streaming\n",
    "\n",
    "This bootcamp will show how we can set up Spark with AWS and use it through Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will build an EC2 (Elastic Computer 2) cloud server on AWS**\n",
    "\n",
    "EC2 is basically a virtual computer that can be accessed through the cloud.\n",
    "\n",
    "We will use AWS and SSH (Secure Shell) to set up the server (most info directly in video and not here)\n",
    "\n",
    "Upon completion we will install PySpark on here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All relevant code for pyspark, RDDs, etc. can be found in the Jupyter notebook in the Spark database**\n",
    "\n",
    "Steps to connect to Spark database from computer:\n",
    "\n",
    "Connect to SSH (bash shell) using Putty (saved in Downloads)\n",
    "\n",
    "-- need to enter email address (ubuntu@amazon_instance) \n",
    "\n",
    "-- also need to authorize using .ppk file saved on desktop\n",
    "\n",
    "Once shell launches, need to write the following 3 lines in Ubuntu so Python knows where Spark is saved:\n",
    "\n",
    "$ export SPARK_HOME='/home/ubuntu/spark-2.0.0-bin-hadoop2.7'\n",
    "\n",
    "$ export PATH=$SPARK_HOME:$PATH\n",
    "\n",
    "$ export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\n",
    "\n",
    "\n",
    "Once this is complete, type Jupyter Notebook into the Ubuntu command line to start running software\n",
    "\n",
    "Jupyter will be available at the following address in a web browser:\n",
    "\n",
    "https://ec2-13-58-226-14.us-east-2.compute.amazonaws.com:8888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See RDD Transformations and Actions file from bootcamp for full list of common transformations and actions in Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
